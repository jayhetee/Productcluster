{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(70 unique tokens: ['extra', 'surface', 'brilliant', 'permanent', 'tip']...)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script\n",
    "* Loads documents as aggregation of tweets stored in a MongoDB collection\n",
    "* Cleans up the documents\n",
    "* Creates a dictionary and corpus that can be used to train an LDA model\n",
    "* Training of the LDA model is not included but follows:\n",
    "  lda = models.LdaModel(corpus, id2word=dictionary, num_topics=100, passes=100)\n",
    "Author: Alex Perrier\n",
    "Python 2.7\n",
    "'''\n",
    "\n",
    "import langid\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from configparser import ConfigParser\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pymongo import MongoClient\n",
    "from string import digits\n",
    "\n",
    "\n",
    "#def filter_lang(lang, documents):\n",
    "#    doclang = [  langid.classify(doc) for doc in documents ]\n",
    "#    return [documents[k] for k in range(len(documents)) if doclang[k][0] == lang]\n",
    "\n",
    "# connect to the MongoDB\n",
    "#client      = MongoClient()\n",
    "#db          = client['twitter']\n",
    "\n",
    "# Load documents and followers from db\n",
    "# Filter out non-english timelines and TL with less than 2 tweets\n",
    "#documents    = [tw['raw_text'] for tw in db.tweets.find()\n",
    "#                    if ('lang' in tw.keys()) and (tw['lang'] in ('en','und'))\n",
    "#                        and ('n_tweets' in tw.keys()) and (tw['n_tweets'] > 2) ]\n",
    "\n",
    "#  Filter non english documents\n",
    "#documents = filter_lang('en', documents)\n",
    "#print(\"We have \" + str(len(documents)) + \" documents in english \")\n",
    "\n",
    "# Remove urls\n",
    "#documents = [re.sub(r\"(?:\\@|http?\\://)\\S+\", \"\", doc)\n",
    "#                for doc in documents ]\n",
    "\n",
    "# Remove documents with less 100 words (some timeline are only composed of URLs)\n",
    "#documents = [doc for doc in documents if len(doc) > 100]\n",
    "data = [line.rstrip() for line in open('data.txt')]\n",
    "import string\n",
    "for i in range(len(data)):\n",
    "    for c in string.punctuation:\n",
    "        data[i]= data[i].replace(c,\"\")       \n",
    "documents=data[:100]\n",
    "# tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "documents = [ tokenizer.tokenize(doc.lower()) for doc in documents ]\n",
    "\n",
    "# Remove stop words\n",
    "stoplist_tw=['amp','get','got','hey','hmm','hoo','hop','iep','let','ooo','par',\n",
    "            'pdt','pln','pst','wha','yep','yer','aest','didn','nzdt','via',\n",
    "            'one','com','new','like','great','make','top','awesome','best',\n",
    "            'good','wow','yes','say','yay','would','thanks','thank','going',\n",
    "            'new','use','should','could','best','really','see','want','nice',\n",
    "            'while','know']\n",
    "\n",
    "unigrams = [ w for doc in documents for w in doc if len(w)==1]\n",
    "bigrams  = [ w for doc in documents for w in doc if len(w)==2]\n",
    "\n",
    "stoplist  = set(nltk.corpus.stopwords.words(\"english\") + stoplist_tw\n",
    "                + unigrams + bigrams)\n",
    "documents = [[token for token in doc if token not in stoplist]\n",
    "                for doc in documents]\n",
    "\n",
    "# rm numbers only words\n",
    "documents = [ [token for token in doc if len(token.strip(digits)) == len(token)]\n",
    "                for doc in documents ]\n",
    "\n",
    "# Lammetization\n",
    "# This did not add coherence ot the model and obfuscates interpretability of the\n",
    "# Topics. It was not used in the final model.\n",
    "#   from nltk.stem import WordNetLemmatizer\n",
    "#   lmtzr = WordNetLemmatizer()\n",
    "#   documents=[[lmtzr.lemmatize(token) for token in doc ] for doc in documents]\n",
    "\n",
    "# Remove words that only occur once\n",
    "token_frequency = defaultdict(int)\n",
    "\n",
    "# count all token\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        token_frequency[token] += 1\n",
    "\n",
    "# keep words that occur more than once\n",
    "documents = [ [token for token in doc if token_frequency[token] > 1]\n",
    "                for doc in documents  ]\n",
    "\n",
    "# Sort words in documents\n",
    "for doc in documents:\n",
    "    doc.sort()\n",
    "\n",
    "# Build a dictionary where for each document each word has its own id\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.compactify()\n",
    "# and save the dictionary for future use\n",
    "dictionary.save('JamesT.dict')\n",
    "\n",
    "# We now have a dictionary with 26652 unique tokens\n",
    "print(dictionary)\n",
    "\n",
    "# Build the corpus: vectors with occurence of each word for each document\n",
    "# convert tokenized documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# and save in Market Matrix format\n",
    "corpora.MmCorpus.serialize('JamesT.mm', corpus)\n",
    "# this corpus can be loaded with corpus = corpora.MmCorpus('alexip_followers.mm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "    level=logging.INFO)\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define KL function\n",
    "def sym_kl(p,q):\n",
    "    return np.sum([stats.entropy(p,q),stats.entropy(q,p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_corpus=corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-8-0a369b4ed8a2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-0a369b4ed8a2>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def arun(corpus1,dictionary,min_topics=1,max_topics,step=1):\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "l = np.array([sum(cnt for _, cnt in doc) for doc in my_corpus])\n",
    "def arun(corpus1,dictionary,min_topics=1,max_topics,step=1):\n",
    "    kl = []\n",
    "    for i in range(min_topics,max_topics,step):\n",
    "        lda = models.ldamodel.LdaModel(corpus=corpus1,\n",
    "            id2word=dictionary,num_topics=i)\n",
    "        m1 = lda.expElogbeta\n",
    "        U,cm1,V = np.linalg.svd(m1)\n",
    "        #Document-topic matrix\n",
    "        lda_topics = lda[my_corpus]\n",
    "        m2 = matutils.corpus2dense(lda_topics, lda.num_topics).transpose()\n",
    "        cm2 = l.dot(m2)\n",
    "        cm2 = cm2 + 0.0001\n",
    "        cm2norm = np.linalg.norm(l)\n",
    "        cm2 = cm2/cm2norm\n",
    "        kl.append(sym_kl(cm1,cm2))\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#documents\n",
    "from gensim import corpora, models, similarities\n",
    "num_topics=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    }
   ],
   "source": [
    "#print ([i for i in lda_corpus])\n",
    "pda = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "corpus_lda = pda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-70c126ce9c1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pda' is not defined"
     ]
    }
   ],
   "source": [
    "pda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster1 = [j for i,j in zip(corpus_lda,documents) if i[0][1] > i[1][1] and i[0][1] > i[2][1]]\n",
    "cluster2 = [j for i,j in zip(corpus_lda,documents) if i[1][1] > i[0][1] and i[1][1] > i[2][1]]\n",
    "cluster3 = [j for i,j in zip(corpus_lda,documents) if i[2][1] > i[1][1] and i[2][1] > i[0][1]]\n",
    "#cluster2 = [j for i,j in zip(lda_corpus,documents) if i[1][1] > threshold]\n",
    "#cluster3 = [j for i,j in zip(lda_corpus,documents) if i[2][1] > threshold]\n",
    "\n",
    "#print (cluster1)\n",
    "#print (cluster2)\n",
    "#print (cluster3)\n",
    "#print(len(cluster1))\n",
    "#print(len(cluster2))\n",
    "#print(len(cluster3))\n",
    "#print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cluster1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
